#!/usr/bin/env python3
"""
æµ‹è¯•MPCç±»çš„å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹
é‡ç‚¹æµ‹è¯•eval_adjoint_solution_sensitivityåŠŸèƒ½
"""

import sys
import os
import torch
import numpy as np
import time
import traceback
try:
    from torch.autograd.gradcheck import gradcheck
except ImportError:
    gradcheck = None

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/zhaoguodong/work/code/MAPPO-MPC-OmniDrones/OmniDrones')
sys.path.append('/home/zhaoguodong/work/code/MAPPO-MPC-OmniDrones/acados')

# å¯¼å…¥MPCç›¸å…³æ¨¡å—
from omni_drones.learning.mpc_components.Mpc import MPC, MpcModule, QuadrotorMpcFunction
from omni_drones.learning.mpc_components.diff_acados import clear_solver_cache

def normalize_quaternion(quat):
    """å½’ä¸€åŒ–å››å…ƒæ•°"""
    norm = torch.norm(quat, dim=-1, keepdim=True)
    return quat / (norm + 1e-8)

def create_test_initial_states(batch_size=4, device="cpu"):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå§‹çŠ¶æ€"""
    # çŠ¶æ€: [px, py, pz, qw, qx, qy, qz, vx, vy, vz]
    states = torch.zeros(batch_size, 10, device=device, dtype=torch.float32)
    
    # è®¾ç½®ä½ç½® (åœ¨åŸç‚¹é™„è¿‘çš„å°æ‰°åŠ¨)
    states[:, 0] = torch.randn(batch_size) * 0.1  # px
    states[:, 1] = torch.randn(batch_size) * 0.1  # py  
    states[:, 2] = torch.randn(batch_size) * 0.1 + 1.0  # pz (æ‚¬åœåœ¨1ç±³é«˜åº¦)
    
    # è®¾ç½®å››å…ƒæ•° (æ¥è¿‘å•ä½å››å…ƒæ•°ï¼Œå°è§’åº¦æ‰°åŠ¨)
    small_angles = torch.randn(batch_size, 3) * 0.1  # å°è§’åº¦æ‰°åŠ¨
    states[:, 3] = torch.cos(torch.norm(small_angles, dim=1) / 2)  # qw
    sin_half = torch.sin(torch.norm(small_angles, dim=1, keepdim=True) / 2)
    states[:, 4:7] = small_angles * sin_half / (torch.norm(small_angles, dim=1, keepdim=True) + 1e-8)
    
    # å½’ä¸€åŒ–å››å…ƒæ•°
    quat = states[:, 3:7]
    states[:, 3:7] = normalize_quaternion(quat)
    
    # è®¾ç½®é€Ÿåº¦ (å°çš„åˆå§‹é€Ÿåº¦)
    states[:, 7] = torch.randn(batch_size) * 0.1  # vx
    states[:, 8] = torch.randn(batch_size) * 0.1  # vy
    states[:, 9] = torch.randn(batch_size) * 0.1  # vz
    
    return states

def create_test_cost_weights(batch_size=4, device="cpu"):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ä»£ä»·æƒé‡"""
    s_dim = 10  # çŠ¶æ€ç»´åº¦
    u_dim = 4   # æ§åˆ¶ç»´åº¦
    
    # åˆ›å»ºQæƒé‡ (çŠ¶æ€æƒé‡)
    Q_base = torch.tensor([
        100.0, 100.0, 100.0,        # ä½ç½®æƒé‡ (px, py, pz)
        10.0, 10.0, 10.0, 10.0,     # å››å…ƒæ•°æƒé‡ (qw, qx, qy, qz)
        10.0, 10.0, 10.0             # é€Ÿåº¦æƒé‡ (vx, vy, vz)
    ], dtype=torch.float32, device=device)
    
    # åˆ›å»ºRæƒé‡ (æ§åˆ¶æƒé‡)
    R_base = torch.tensor([0.1, 0.1, 0.1, 0.1], dtype=torch.float32, device=device)
    
    # ä¸ºæ¯ä¸ªbatchæ·»åŠ å°çš„éšæœºæ‰°åŠ¨
    Q_diag = Q_base.unsqueeze(0).repeat(batch_size, 1)
    R_diag = R_base.unsqueeze(0).repeat(batch_size, 1)
    
    # æ·»åŠ å°çš„éšæœºæ‰°åŠ¨ä½¿æ¯ä¸ªbatchçš„æƒé‡ç•¥æœ‰ä¸åŒ
    Q_diag += torch.randn_like(Q_diag) * 0.01 * Q_base.unsqueeze(0)
    R_diag += torch.randn_like(R_diag) * 0.01 * R_base.unsqueeze(0)
    
    # ç¡®ä¿æƒé‡ä¸ºæ­£
    Q_diag = torch.abs(Q_diag)
    R_diag = torch.abs(R_diag)
    
    # åˆå¹¶Qå’ŒRæƒé‡
    Q_q = torch.cat([Q_diag, R_diag], dim=1)
    
    return Q_q

def test_forward_pass(mpc, batch_size=4, device="cpu"):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•å‰å‘ä¼ æ’­ (Forward Pass)")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x0 = create_test_initial_states(batch_size, device)
        Q_q = create_test_cost_weights(batch_size, device)
        
        print(f"åˆå§‹çŠ¶æ€å½¢çŠ¶: {x0.shape}")
        print(f"ä»£ä»·æƒé‡å½¢çŠ¶: {Q_q.shape}")
        print(f"åˆå§‹çŠ¶æ€ç¤ºä¾‹:\n{x0[0]}")
        print(f"ä»£ä»·æƒé‡ç¤ºä¾‹:\n{Q_q[0]}")
        
        # æµ‹è¯•éè®­ç»ƒæ¨¡å¼
        print("\n--- æµ‹è¯•éè®­ç»ƒæ¨¡å¼ (is_training=False) ---")
        start_time = time.time()
        opt_u_eval, x_pred_eval = mpc.solve(Q_q, x0, is_training=False)
        forward_time_eval = time.time() - start_time
        
        print(f"å‰å‘ä¼ æ’­æ—¶é—´ (éè®­ç»ƒ): {forward_time_eval:.4f}ç§’")
        print(f"æœ€ä¼˜æ§åˆ¶å½¢çŠ¶: {opt_u_eval.shape}")
        print(f"é¢„æµ‹è½¨è¿¹å½¢çŠ¶: {x_pred_eval.shape}")
        print(f"æœ€ä¼˜æ§åˆ¶ç¤ºä¾‹:\n{opt_u_eval[0]}")
        
        # æ£€æŸ¥æ§åˆ¶çº¦æŸ
        thrust_min, thrust_max = 2.0, 20.0
        w_max_xy, w_max_yaw = 6.0, 6.0
        
        assert torch.all(opt_u_eval[:, 0] >= thrust_min), "æ¨åŠ›ä¸‹ç•Œè¿å"
        assert torch.all(opt_u_eval[:, 0] <= thrust_max), "æ¨åŠ›ä¸Šç•Œè¿å"
        assert torch.all(opt_u_eval[:, 1:3] >= -w_max_xy), "xyè§’é€Ÿåº¦ä¸‹ç•Œè¿å"
        assert torch.all(opt_u_eval[:, 1:3] <= w_max_xy), "xyè§’é€Ÿåº¦ä¸Šç•Œè¿å"
        assert torch.all(opt_u_eval[:, 3] >= -w_max_yaw), "yawè§’é€Ÿåº¦ä¸‹ç•Œè¿å"
        assert torch.all(opt_u_eval[:, 3] <= w_max_yaw), "yawè§’é€Ÿåº¦ä¸Šç•Œè¿å"
        
        print("âœ“ æ§åˆ¶çº¦æŸæ£€æŸ¥é€šè¿‡")
        
        # æµ‹è¯•è®­ç»ƒæ¨¡å¼
        print("\n--- æµ‹è¯•è®­ç»ƒæ¨¡å¼ (is_training=True) ---")
        start_time = time.time()
        opt_u_train, x_pred_train = mpc.solve(Q_q, x0, is_training=True)
        forward_time_train = time.time() - start_time
        
        print(f"å‰å‘ä¼ æ’­æ—¶é—´ (è®­ç»ƒ): {forward_time_train:.4f}ç§’")
        print(f"æœ€ä¼˜æ§åˆ¶å½¢çŠ¶: {opt_u_train.shape}")
        print(f"é¢„æµ‹è½¨è¿¹å½¢çŠ¶: {x_pred_train.shape}")
        print(f"æœ€ä¼˜æ§åˆ¶ç¤ºä¾‹:\n{opt_u_train[0]}")
        
        # æ¯”è¾ƒä¸¤ç§æ¨¡å¼çš„ç»“æœå·®å¼‚
        u_diff = torch.norm(opt_u_train - opt_u_eval)
        x_diff = torch.norm(x_pred_train - x_pred_eval)
        
        print(f"\næ§åˆ¶å·®å¼‚ (è®­ç»ƒ vs éè®­ç»ƒ): {u_diff:.6f}")
        print(f"è½¨è¿¹å·®å¼‚ (è®­ç»ƒ vs éè®­ç»ƒ): {x_diff:.6f}")
        
        if u_diff < 1e-3 and x_diff < 1e-3:
            print("âœ“ ä¸¤ç§æ¨¡å¼ç»“æœä¸€è‡´")
        else:
            print("âš  ä¸¤ç§æ¨¡å¼ç»“æœå­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½æ˜¯æ±‚è§£å™¨è®¾ç½®ä¸åŒ")
        
        print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
        return opt_u_train, x_pred_train, Q_q, x0
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None, None, None

def test_backward_pass(mpc, opt_u, x_pred, Q_q, x0, device="cpu"):
    """æµ‹è¯•åå‘ä¼ æ’­"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•åå‘ä¼ æ’­ (Backward Pass)")
    print("=" * 60)
    
    try:
        # è®¾ç½®éœ€è¦æ¢¯åº¦çš„å˜é‡
        Q_q_grad = Q_q.clone().detach().requires_grad_(True)
        x0_grad = x0.clone().detach().requires_grad_(True)
        
        print(f"Q_qæ¢¯åº¦å¯ç”¨: {Q_q_grad.requires_grad}")
        print(f"x0æ¢¯åº¦å¯ç”¨: {x0_grad.requires_grad}")
        
        # å‰å‘ä¼ æ’­ (è®­ç»ƒæ¨¡å¼)
        print("\n--- æ‰§è¡Œè®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­ ---")
        start_time = time.time()
        opt_u_train, x_pred_train = mpc.solve(Q_q_grad, x0_grad, is_training=True)
        forward_time = time.time() - start_time
        print(f"å‰å‘ä¼ æ’­æ—¶é—´: {forward_time:.4f}ç§’")
        
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°
        # ç›®æ ‡: æœ€å°åŒ–æ§åˆ¶èƒ½é‡å’Œä½ç½®è¯¯å·®
        target_pos = torch.zeros_like(x_pred_train[0, :, :3])  # ç›®æ ‡ä½ç½®ä¸ºåŸç‚¹
        target_u = torch.tensor([9.81, 0.0, 0.0, 0.0], device=device).expand_as(opt_u_train)
        
        # æŸå¤± = æ§åˆ¶è¯¯å·® + æœ€ç»ˆä½ç½®è¯¯å·®
        control_loss = torch.sum((opt_u_train - target_u) ** 2)
        position_loss = torch.sum((x_pred_train[-1, :, :3] - target_pos) ** 2)
        total_loss = control_loss + position_loss
        
        print(f"æ§åˆ¶æŸå¤±: {control_loss.item():.6f}")
        print(f"ä½ç½®æŸå¤±: {position_loss.item():.6f}")
        print(f"æ€»æŸå¤±: {total_loss.item():.6f}")
        
        # åå‘ä¼ æ’­
        print("\n--- æ‰§è¡Œåå‘ä¼ æ’­ ---")
        start_time = time.time()
        total_loss.backward()
        backward_time = time.time() - start_time
        print(f"åå‘ä¼ æ’­æ—¶é—´: {backward_time:.4f}ç§’")
        
        # æ£€æŸ¥æ¢¯åº¦
        print("\n--- æ¢¯åº¦æ£€æŸ¥ ---")
        if Q_q_grad.grad is not None:
            q_grad_norm = torch.norm(Q_q_grad.grad)
            q_grad_max = torch.max(torch.abs(Q_q_grad.grad))
            print(f"Q_qæ¢¯åº¦èŒƒæ•°: {q_grad_norm:.6f}")
            print(f"Q_qæ¢¯åº¦æœ€å¤§å€¼: {q_grad_max:.6f}")
            print(f"Q_qæ¢¯åº¦å½¢çŠ¶: {Q_q_grad.grad.shape}")
            print(f"Q_qæ¢¯åº¦ç¤ºä¾‹ (å‰5ä¸ªå…ƒç´ ):\n{Q_q_grad.grad[0, :5]}")
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åˆç†
            if torch.any(torch.isnan(Q_q_grad.grad)):
                print("âŒ Q_qæ¢¯åº¦åŒ…å«NaN")
                return False
            elif torch.any(torch.isinf(Q_q_grad.grad)):
                print("âŒ Q_qæ¢¯åº¦åŒ…å«Inf")
                return False
            elif q_grad_norm < 1e-10:
                print("âš  Q_qæ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
            else:
                print("âœ“ Q_qæ¢¯åº¦æ­£å¸¸")
        else:
            print("âŒ Q_qæ¢¯åº¦ä¸ºNone")
            return False
        
        if x0_grad.grad is not None:
            x0_grad_norm = torch.norm(x0_grad.grad)
            x0_grad_max = torch.max(torch.abs(x0_grad.grad))
            print(f"x0æ¢¯åº¦èŒƒæ•°: {x0_grad_norm:.6f}")
            print(f"x0æ¢¯åº¦æœ€å¤§å€¼: {x0_grad_max:.6f}")
            print(f"x0æ¢¯åº¦å½¢çŠ¶: {x0_grad.grad.shape}")
            print(f"x0æ¢¯åº¦ç¤ºä¾‹:\n{x0_grad.grad[0]}")
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åˆç†
            if torch.any(torch.isnan(x0_grad.grad)):
                print("âŒ x0æ¢¯åº¦åŒ…å«NaN")
                return False
            elif torch.any(torch.isinf(x0_grad.grad)):
                print("âŒ x0æ¢¯åº¦åŒ…å«Inf")
                return False
            else:
                print("âœ“ x0æ¢¯åº¦æ­£å¸¸")
        else:
            print("âš  x0æ¢¯åº¦ä¸ºNone (å¯èƒ½æœªå®ç°)")
        
        print("âœ“ åå‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ åå‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_gradient_consistency(mpc, device="cpu", batch_size=2):
    """æµ‹è¯•æ¢¯åº¦ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ¢¯åº¦ä¸€è‡´æ€§")
    print("=" * 60)
    
    try:
        # åˆ›å»ºå°batchçš„æµ‹è¯•æ•°æ®
        x0 = create_test_initial_states(batch_size, device)
        Q_q = create_test_cost_weights(batch_size, device)
        
        # è®¾ç½®éœ€è¦æ¢¯åº¦
        Q_q.requires_grad_(True)
        
        # å®šä¹‰ä¸€ä¸ªåŒ…è£…å‡½æ•°ç”¨äºæ¢¯åº¦æ£€æŸ¥
        def mpc_wrapper(Q_q_input):
            opt_u, _ = mpc.solve(Q_q_input, x0, is_training=True)
            return torch.sum(opt_u ** 2)  # ç®€å•çš„äºŒæ¬¡æŸå¤±
        
        print("æ‰§è¡Œæ•°å€¼æ¢¯åº¦æ£€æŸ¥...")
        
        # ä½¿ç”¨è¾ƒå¤§çš„epsilonå› ä¸ºMPCæ±‚è§£å™¨çš„æ•°å€¼ç²¾åº¦æœ‰é™
        eps = 1e-3
        
        # æ‰‹åŠ¨è®¡ç®—æ•°å€¼æ¢¯åº¦ (åªæ£€æŸ¥å‡ ä¸ªé‡è¦å‚æ•°)
        print("è®¡ç®—è§£ææ¢¯åº¦...")
        Q_q_test = Q_q.clone().detach().requires_grad_(True)
        loss_analytical = mpc_wrapper(Q_q_test)
        loss_analytical.backward()
        
        if Q_q_test.grad is not None:
            analytical_grad = Q_q_test.grad.clone()
        else:
            print("âŒ è§£ææ¢¯åº¦ä¸ºNone")
            return False
        
        print("è®¡ç®—æ•°å€¼æ¢¯åº¦...")
        numerical_grad = torch.zeros_like(Q_q)
        
        # åªæ£€æŸ¥å‰å‡ ä¸ªå‚æ•°ä»¥èŠ‚çœæ—¶é—´
        indices_to_check = [(0, 0), (0, 1), (0, 10), (0, 11)]  # æ£€æŸ¥ä¸€äº›ä»£è¡¨æ€§çš„å‚æ•°
        
        for i, j in indices_to_check:
            if i < Q_q.shape[0] and j < Q_q.shape[1]:
                # æ­£å‘æ‰°åŠ¨
                Q_q_plus = Q_q.clone().detach()
                Q_q_plus[i, j] += eps
                loss_plus = mpc_wrapper(Q_q_plus)
                
                # è´Ÿå‘æ‰°åŠ¨
                Q_q_minus = Q_q.clone().detach()
                Q_q_minus[i, j] -= eps
                loss_minus = mpc_wrapper(Q_q_minus)
                
                # ä¸­å¿ƒå·®åˆ†
                numerical_grad[i, j] = (loss_plus - loss_minus) / (2 * eps)
                
                # æ¯”è¾ƒ
                analytical_val = analytical_grad[i, j].item()
                numerical_val = numerical_grad[i, j].item()
                relative_error = abs(analytical_val - numerical_val) / (abs(analytical_val) + abs(numerical_val) + 1e-8)
                
                print(f"å‚æ•°[{i},{j}]: è§£æ={analytical_val:.6f}, æ•°å€¼={numerical_val:.6f}, ç›¸å¯¹è¯¯å·®={relative_error:.4f}")
                
                if relative_error < 0.1:  # 10%çš„å®¹å¿åº¦ï¼Œå› ä¸ºMPCæ±‚è§£å™¨çš„æ•°å€¼ç‰¹æ€§
                    print(f"âœ“ å‚æ•°[{i},{j}]æ¢¯åº¦æ£€æŸ¥é€šè¿‡")
                else:
                    print(f"âš  å‚æ•°[{i},{j}]æ¢¯åº¦æ£€æŸ¥æœ‰è¾ƒå¤§è¯¯å·®")
        
        print("âœ“ æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_sensitivity_computation_directly():
    """ç›´æ¥æµ‹è¯•çµæ•åº¦è®¡ç®—åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ç›´æ¥æµ‹è¯•eval_adjoint_solution_sensitivity")
    print("=" * 60)
    
    try:
        from omni_drones.learning.mpc_components.diff_acados import solve_using_acados
        from omni_drones.learning.mpc_components.problems import (
            NonlinearDiscreteDynamics, QuadraticCost, ControlBounds, ControlBoundedOcp
        )
        import casadi as ca
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•é—®é¢˜
        batch_size = 2
        s_dim = 10
        u_dim = 4
        N_horizon = 5
        dt = 0.05
        
        # åˆ›å»ºCasADiå˜é‡
        ca_x = ca.SX.sym('x', s_dim)
        ca_u = ca.SX.sym('u', u_dim)
        
        # ç®€å•çš„çº¿æ€§åŠ¨åŠ›å­¦ (ç”¨äºæµ‹è¯•)
        A = np.eye(s_dim) + np.random.randn(s_dim, s_dim) * 0.01
        B = np.random.randn(s_dim, u_dim) * 0.1
        
        # åˆ›å»ºCasADiè¡¨è¾¾å¼
        f_expr = ca.mtimes(A, ca_x) + ca.mtimes(B, ca_u)
        
        # åˆ›å»ºåŠ¨åŠ›å­¦å¯¹è±¡ (ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è™šæ‹ŸPyTorchæ¨¡å‹)
        class DummyDynamics(torch.nn.Module):
            def forward(self, x, u):
                return x  # ç®€å•è¿”å›
        
        dummy_model = DummyDynamics()
        
        dynamics = NonlinearDiscreteDynamics(
            f_pytorch=dummy_model,
            f_casadi_expr=f_expr,
            x=ca_x,
            u=ca_u,
            dt=dt
        )
        
        # åˆ›å»ºæˆæœ¬å‡½æ•°
        cost = QuadraticCost(
            Q=np.eye(s_dim) * 10.0,
            R=np.eye(u_dim) * 0.1,
            r=np.zeros(s_dim),
            q=np.zeros(u_dim)
        )
        
        # åˆ›å»ºæ§åˆ¶çº¦æŸ
        control_bounds = ControlBounds(
            u_lower=np.array([-10.0] * u_dim),
            u_upper=np.array([10.0] * u_dim)
        )
        
        # åˆ›å»ºOCPé—®é¢˜
        problem = ControlBoundedOcp(
            dynamics=dynamics,
            cost=cost,
            control_bounds=control_bounds,
            N_horizon=N_horizon
        )
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        x0_vals = np.random.randn(batch_size, s_dim) * 0.1
        
        # åˆ›å»ºä»£ä»·æƒé‡
        Q_diag_np = np.ones((batch_size, s_dim)) * 10.0
        R_diag_np = np.ones((batch_size, u_dim)) * 0.1
        
        # æµ‹è¯•ä¸å¸¦çµæ•åº¦çš„æ±‚è§£
        print("æµ‹è¯•ä¸å¸¦çµæ•åº¦çš„æ±‚è§£...")
        x_sol_no_sens, u_sol_no_sens, timing_no_sens, sens_no_sens = solve_using_acados(
            problem, x0_vals, Q_diag_np, R_diag_np,
            seed=None,
            batched=True,
            solver_options={
                "nlp_solver_type": "SQP",
                "qp_solver": "PARTIAL_CONDENSING_HPIPM",
                "nlp_solver_max_iter": 10,
                "print_level": 0,
                "integrator_type": "DISCRETE"
            }
        )
        print(f"âœ“ ä¸å¸¦çµæ•åº¦æ±‚è§£æˆåŠŸï¼Œè€—æ—¶: {timing_no_sens:.4f}ç§’")
        
        # æµ‹è¯•å¸¦çµæ•åº¦çš„æ±‚è§£
        print("\næµ‹è¯•å¸¦çµæ•åº¦çš„æ±‚è§£...")
        seed = np.ones(u_dim)
        
        solver_options_sens = {
            "nlp_solver_type": "SQP",
            "qp_solver": "PARTIAL_CONDENSING_HPIPM",
            "nlp_solver_max_iter": 10,
            "print_level": 0,
            "integrator_type": "DISCRETE",
            "with_solution_sens_wrt_params": True,
            "hessian_approx": "EXACT",
            "qp_solver_cond_ric_alg": 0,
            "qp_solver_ric_alg": 1,
            "levenberg_marquardt": 0.0,
        }
        
        x_sol_sens, u_sol_sens, timing_sens, sens_result = solve_using_acados(
            problem, x0_vals, Q_diag_np, R_diag_np,
            seed=seed,
            batched=True,
            solver_options=solver_options_sens
        )
        
        print(f"âœ“ å¸¦çµæ•åº¦æ±‚è§£æˆåŠŸï¼Œè€—æ—¶: {timing_sens:.4f}ç§’")
        
        # æ£€æŸ¥çµæ•åº¦ç»“æœ
        if sens_result is not None:
            print(f"çµæ•åº¦å½¢çŠ¶: {sens_result.shape}")
            print(f"çµæ•åº¦èŒƒæ•°: {np.linalg.norm(sens_result):.6f}")
            print(f"çµæ•åº¦æœ€å¤§å€¼: {np.max(np.abs(sens_result)):.6f}")
            
            if np.any(np.isnan(sens_result)):
                print("âŒ çµæ•åº¦åŒ…å«NaN")
                return False
            elif np.any(np.isinf(sens_result)):
                print("âŒ çµæ•åº¦åŒ…å«Inf")
                return False
            else:
                print("âœ“ çµæ•åº¦è®¡ç®—æ­£å¸¸")
        else:
            print("âŒ çµæ•åº¦ç»“æœä¸ºNone")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›´æ¥çµæ•åº¦æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def run_comprehensive_test():
    """è¿è¡Œå…¨é¢çš„æµ‹è¯•"""
    print("å¼€å§‹MPCå¯å¾®åˆ†æ€§å…¨é¢æµ‹è¯•")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = "cpu"  # ä½¿ç”¨CPUç¡®ä¿ç¨³å®šæ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æ¸…é™¤ä¹‹å‰çš„æ±‚è§£å™¨ç¼“å­˜
    clear_solver_cache()
    
    # åˆ›å»ºMPCå®ä¾‹
    T = 0.25  # é¢„æµ‹æ—¶åŸŸ
    dt = 0.05  # æ—¶é—´æ­¥é•¿
    batch_size = 4
    
    print(f"åˆ›å»ºMPCå®ä¾‹ (T={T}, dt={dt}, device={device})")
    mpc = MPC(T=T, dt=dt, device=device)
    
    # æµ‹è¯•åºåˆ—
    tests_passed = 0
    total_tests = 4
    
    # 1. æµ‹è¯•å‰å‘ä¼ æ’­
    opt_u, x_pred, Q_q, x0 = test_forward_pass(mpc, batch_size, device)
    if opt_u is not None:
        tests_passed += 1
    
    # 2. æµ‹è¯•åå‘ä¼ æ’­ (å¦‚æœå‰å‘ä¼ æ’­æˆåŠŸ)
    if opt_u is not None:
        if test_backward_pass(mpc, opt_u, x_pred, Q_q, x0, device):
            tests_passed += 1
    
    # 3. æµ‹è¯•æ¢¯åº¦ä¸€è‡´æ€§
    if test_gradient_consistency(mpc, device, batch_size=2):
        tests_passed += 1
    
    # 4. ç›´æ¥æµ‹è¯•çµæ•åº¦è®¡ç®—
    if test_sensitivity_computation_directly():
        tests_passed += 1
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 80)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 80)
    print(f"é€šè¿‡çš„æµ‹è¯•: {tests_passed}/{total_tests}")
    
    if tests_passed == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MPCå¯å¾®åˆ†æ€§å·¥ä½œæ­£å¸¸ã€‚")
        return True
    else:
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        return False

if __name__ == "__main__":
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)
